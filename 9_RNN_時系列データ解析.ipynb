{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "9_RNN_時系列データ解析.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgZWP1-N614t",
        "colab_type": "text"
      },
      "source": [
        "# 実習9\n",
        "#### 時系列データ解析\n",
        "\n",
        "#### 参考\n",
        "PythonとKerasによるディープラーニング\n",
        "(https://www.amazon.co.jp/PythonとKerasによるディープラーニング-Francois-Chollet/dp/4839964262)　6章\n",
        "\n",
        "気象時系列データセットを使用する。これはドイツのイエナにあるMax Planck Institute for Biogeochemistryの観測所で記録されたデータである。このデータセットは14種類の数値(気温、気圧、湿度、風向など)を10分おきに記録した数年分のデータで構成されている。最も古いデータは2003年のものだが、ここでは2009年から2016年のデータのみを使用する。このデータを使用して、最近の様々な気象データ(10日分のデータ)を入力として受け取り、24時間後の気温を予測するモデルを構築する。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZbG7n3n614w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cd ~/Downloads\n",
        "!mkdir jena_climate\n",
        "!cd jena_climate\n",
        "!wget https://s3.amazonaws.com/keras-datasets/jena_climate_2009_2016.csv.zip\n",
        "!unzip jena_climate_2009_2016.csv.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiVQ9UFx6145",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 今日データセットの中身をみる\n",
        "import os\n",
        "\n",
        "#データセットが置かれているディレクトリ\n",
        "data_dir = ''\n",
        "fname = os.path.join(data_dir, 'jena_climate_2009_2016.csv')\n",
        "\n",
        "#データの読み込み\n",
        "f = open(fname)\n",
        "data = f.read() \n",
        "f.close()\n",
        "\n",
        "\n",
        "lines = data.split('\\n')       #行ごとに改行で区切って\n",
        "header = lines[0].split(',')   #0行目はheaderとして取り出す\n",
        "lines = lines[1:]              #1行目以降をlinesに格納する\n",
        "\n",
        "print(header) #CSVファイルの0行目、Timestamp + 14種類の気象情報を見る。\n",
        "print(len(lines)) #各時系列データの長さ\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHScqfGM614-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#01.01.2009 00:10:00から00:50:00までのデータ(0行目から4行目)を見る\n",
        "lines[0:5] "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cxbefbH615D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#420551行の時系列データ(ベクトル)をNumPy配列(2次元テンソル)に変換\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "float_data = np.zeros((len(lines), len(header) - 1)) #Timestampを消すので-1する\n",
        "for i, line in enumerate(lines):\n",
        "    values = [float(x) for x in line.split(',')[1:]] #Timestampを消すので[1:]で0列目を排除\n",
        "    float_data[i, :] = values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGb2JhdR615G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# float_dataが420551行x14列の2次元行列データになっていることを確認\n",
        "print(float_data.shape)\n",
        "print(float_data[0:5])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBcd5hOI615I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 気温の時系列データのプロット (2009年から2016年のデータ、8年分)\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "temp = float_data[:, 1] #気温(T (degC))の列のみ取り出し\n",
        "plt.plot(range(len(temp)), temp)\n",
        "plt.show\n",
        "\n",
        "\"\"\"\n",
        "8年分のデータなので、8個のピーク(夏)が見れる。\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYzxMAWC615L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 最初の10日間の気温をデータをプロット (10分おきのデータなので10日分は、(60/10)*24*10 = 1440)\n",
        "plt.plot(range(1440), temp[:1440])\n",
        "plt.show()\n",
        "\n",
        "\"\"\"\n",
        "かなり寒い時期の10日間のデータのよう。最後の4日間は24時間の周期が見れます。\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NIz0qvi615N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# データの正規化\n",
        "\n",
        "\"\"\"\n",
        "気温は一般的に-20から+30度程度の値だが、気圧はミリバール単位であり、1000前後の値になります。\n",
        "そこで、各時系列を個別に正規化し、全ての時系列が同じような尺度の小さな値になるようにする。\n",
        "訓練データとして最初の20万個のデータを使うので、この部分を正規化する。\n",
        "\"\"\"\n",
        "\n",
        "#平均値を引く\n",
        "mean = float_data[:200000].mean(axis=0)\n",
        "float_data -= mean\n",
        "\n",
        "#分散で割り算\n",
        "std = float_data[:200000].std(axis=0)\n",
        "float_data /= std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xy3DwKOZ615R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# データの準備。(時系列サンプルとそれらの目的値を生成するジェネレータ)\n",
        "# 入力データのバッチ(samples)とその入力データに対応する目的値(気温)(targets)のタプル(smples, targets)を生成\n",
        "\n",
        "\"\"\"\n",
        "このデータセットのサンプルはかなり冗長であるため(サンプルNとサンプルN+1のデータがほとんど変わらない)\n",
        "すべてのサンプルを明示的に割り当てることは非常に無駄であるため、代わりに元のデータを使用してその場でサンプルを生成する。\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "data: 一つ前のcellで正規化したdata\n",
        "lookback: 入力データの時刻刻みをいくつ遡るか。10日分のデータを入力して予測に使うので、後で1440 (=6*24*10)に設定する。\n",
        "delay: 目的値の時刻刻みをいくつ進めるか。今、24時間後のデータ予測を考えているので後で144 (=6*24)に設定する。\n",
        "min_index, max_index: 抽出する時間刻みの上限と下限を表すdata配列のインデックス。データの一部を検証とテストのために取っておくのに役立つ\n",
        "shuffle: サンプルをシャッフルするのか、それとも時間の順序で抽出するのかを決める\n",
        "step: データをサンプリングするときの期間。データ点を1時間ごとに1つ抽出するため6に設定する。\n",
        "\"\"\"\n",
        "\n",
        "def generator(data, lookback, delay, min_index, max_index, shuffle=False, batch_size=128, step=6):\n",
        "\n",
        "    if max_index is None:\n",
        "        max_index = len(data) - delay -1     #最大より1日分手前までのデータしか使わない\n",
        "    i = min_index + lookback                #最低でも10日分を予測用に確保\n",
        "\n",
        "    while 1:\n",
        "        if shuffle:\n",
        "            rows = np.random.randint(min_index + lookback, max_index, size=batch_size)\n",
        "            #np.random.randint(a, b, size = c)はaからb-1までの整数がランダムに入ったc次元のベクトルが返ってくる\n",
        "\n",
        "        else:\n",
        "            if i + batch_size >= max_index:\n",
        "                i = min_index + lookback\n",
        "            rows = np.arange(i, min(i + batch_size, max_index)) #rowsにはiからi+batch_sizeもしくは、iからmax_index - 1までの整数が入る。だいたい前者\n",
        "            i += len(rows) # len(rows)はだいたいbatch_size\n",
        "\n",
        "        samples = np.zeros((len(rows),\n",
        "                            lookback // step,  # lookback // stepは240 (10日分)\n",
        "                            data.shape[-1]))   # data.shape[-1]は14\n",
        "        targets = np.zeros((len(rows),))\n",
        "        for j, row in enumerate(rows):\n",
        "            indices = range(rows[j] - lookback, rows[j], step) # j = 0の時、rows[0] - lookback = min_indexから, rows[0] = i = min_index + lookbackまでstepで刻む\n",
        "            samples[j] = data[indices]                         # step刻みで10日分のデータを抽出\n",
        "            targets[j] = data[rows[j] + delay][1]              # targetとして気温のみを抽出\n",
        "        yield samples, targets\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QuMufQdQ615U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#訓練、検証、テストに使用するデータジェネレータの準備\n",
        "\n",
        "lookback = 1440   #10日\n",
        "delay = 144       #1日\n",
        "step = 6          # 1時間\n",
        "batch_size = 128\n",
        "\n",
        "# 訓練データジェネレータ\n",
        "train_gen = generator(float_data,\n",
        "                      lookback = lookback,\n",
        "                      delay = delay,\n",
        "                      min_index = 0,\n",
        "                      max_index = 200000,\n",
        "                      shuffle = True,\n",
        "                      step = step,\n",
        "                      batch_size = batch_size)\n",
        "\n",
        "# 検証データジェネレータ\n",
        "val_gen = generator(float_data,\n",
        "                    lookback = lookback,\n",
        "                    delay = delay,\n",
        "                    min_index = 200001,\n",
        "                    max_index = 300000,\n",
        "                    step = step,\n",
        "                    batch_size = batch_size)\n",
        "\n",
        "# テストデータジェネレータ\n",
        "test_gen = generator(float_data,\n",
        "                      lookback = lookback,\n",
        "                      delay = delay,\n",
        "                      min_index = 300001,\n",
        "                      max_index = None,\n",
        "                      step = step,\n",
        "                      batch_size = batch_size)\n",
        "\n",
        "# 検証データセット全体を調べるためにval_genから抽出する時間刻みの数 (= 769)\n",
        "val_steps = (300000 - 200001 - lookback) // batch_size\n",
        "\n",
        "# テストデータセット全体を調べるためにtest_genから抽出する時間刻みの数 (= 930)\n",
        "test_steps = (len(float_data) - 300001 - lookback) // batch_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2n79WqsM615W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjhz0sbh615Y",
        "colab_type": "text"
      },
      "source": [
        "### 機械学習を行う前に、ベースラインの設定\n",
        "ここまででデータセットの準備ができた。ここから24時間後の気温の予測を行なっていく。機械学習による解析を行う前に、ベースラインの設定を行う。気温には24時間単位の周期性があると考えることができる。この常識的な感覚を用いて24時間後の気温が現在と同じ気温になると予測してみる。少なくともこの値を超えなければ機械学習を用いる意味がない。平均絶対誤差(MAE(Mean Absolute Error))を指標としてこのアプローチを評価してみる。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTJne_R2615Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_naive_method():\n",
        "    batch_maes = []\n",
        "    for step in range(val_steps):\n",
        "        samples, targets = next(val_gen)        # val_genのyieldで返されるデータを順番に抽出\n",
        "        preds = samples[:, -1, 1]               # 10日分のデータの最後の気温をpredsとして抽出\n",
        "        mae = np.mean(np.abs(preds - targets))  # 平均絶対誤差を計算\n",
        "        batch_maes.append(mae)\n",
        "    print(np.mean(batch_maes))\n",
        "\n",
        "evaluate_naive_method()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWaJqy-a615c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 上記の値は平均0分散1に正規化されているため、元に戻します。\n",
        "celsius_mae = 0.29*std[1]\n",
        "celsius_mae\n",
        "\n",
        "# 平均絶対誤差はおよそ2.57度になった。機械学習を使ってこれを超える必要がある。"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGbpX15I615h",
        "colab_type": "text"
      },
      "source": [
        "#### 機械学習の基本的アプローチ\n",
        "RNNなどの複雑なモデルを試す前にまずは基本的なモデルで試してみる。複雑なモデルを導入する根拠にもなる。\n",
        "データを平坦化した上で2つのDense層で処理します。最後のDense層では活性化関数が設定されていないが回帰ではこれが一般的。損失関数として平均絶対誤差(MAE)を設定しているため、評価に使用するデータと指標は、上で行った常識的なアプローチと全く同じ。そのため、結果を直接比較することができる。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ALcSvO0615j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras.optimizers import RMSprop\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.Flatten(input_shape = (lookback //step, float_data.shape[-1]))) #10日分のデータを入力\n",
        "\n",
        "model.add(layers.Dense(32, activation='relu'))\n",
        "model.add(layers.Dense(1))\n",
        "\n",
        "model.compile(optimizer = RMSprop(), loss ='mae')\n",
        "hist = model.fit_generator(train_gen,\n",
        "                           steps_per_epoch=500, #1epochで用いるminibatchの数\n",
        "                           epochs=20,\n",
        "                           validation_data=val_gen,\n",
        "                           validation_steps=val_steps)\n",
        "\n",
        "\"\"\"\n",
        "これまで使用してきたmodel.fit()は、データがメモリにのるくらいの規模のデータに向いており、model.fit_generator()は、\n",
        "generatorを使ってバッチ毎にデータを読み込む。メモリ以上のデータを扱うときやバッチ毎に処理をさせるときに使う。\n",
        "画像を扱う際には、ImageDataGeneratorという便利なgeneratorがKerasでは用意されてる。\n",
        "画像以外や少し複雑なことをやりたいときは、自分で準備したgeneratorを使うこともできる。\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQGtRlDK615n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "loss = hist.history['loss']\n",
        "val_loss = hist.history['val_loss']\n",
        "\n",
        "# lossのグラフ\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(range(20), loss, marker='.', label='training_loss')\n",
        "plt.plot(range(20), val_loss, marker='.', label='validation_loss')\n",
        "plt.legend(loc='best', fontsize=10)\n",
        "plt.grid()\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "\n",
        "\"\"\"\n",
        "訓練誤差は下がるが、validation_lossは0.3より悪い値。\n",
        "ベースラインを超えることができていません。『常識』には機械学習モデルが学習することができない貴重な情報が山ほど含まれている。\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SJxr17Q615p",
        "colab_type": "text"
      },
      "source": [
        "#### GRUを使った学習\n",
        "先ほどのアプローチではデータを平坦化することでデータから時間の概念を取り除いた。\n",
        "ここでは、データを時系列データとしてそのままの状態で調べてみることにする。\n",
        "また、LSTMの代わりにGRU(Gated Recurrent Unit)層を使用する。\n",
        "GRU層はLSTMと同様の原理に基づいているが、少し効率化されているため、LSTMほどコストがかからない。(ただし表現力は劣ります。)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UPaneiO615q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, GRU\n",
        "from keras.optimizers import RMSprop\n",
        "\n",
        "model = Sequential()\n",
        "model.add(GRU(32, input_shape=(None, float_data.shape[-1]))) # このNoneは任意の整数を期待する。ジェネレータにより10日分のデータが生成される為Noneを240にしてもOK\n",
        "model.add(Dense(1))\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer=RMSprop(), loss='mae')\n",
        "hist = model.fit_generator(train_gen,\n",
        "                           steps_per_epoch=500, #1epochで用いるminibatchの数\n",
        "                           epochs=20,\n",
        "                           validation_data=val_gen,\n",
        "                           validation_steps=val_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7Nwysoc615s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "loss = hist.history['loss']\n",
        "val_loss = hist.history['val_loss']\n",
        "\n",
        "# lossのグラフ\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(range(20), loss, marker='.', label='training_loss')\n",
        "plt.plot(range(20), val_loss, marker='.', label='validation_loss')\n",
        "plt.legend(loc='best', fontsize=10)\n",
        "plt.grid()\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "\n",
        "\"\"\"\n",
        "ベースラインを少しだけ上回る結果が出てる。\n",
        "やや過学習気味なのでドロップアウトなど入れて過学習を抑えると良くなると考えることができる。\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdqUKuIu615x",
        "colab_type": "text"
      },
      "source": [
        "#### GRU + dropoutによる学習\n",
        "\n",
        "過学習を抑えるためにdropoutを導入する。\n",
        "Kerasには2つのdropout関連パラメータが設定されている。\n",
        "1つはその層の入力ユニットのdropout率を指定する。\n",
        "もう1つはrecurrent_dropoutであり、リカレントユニットのdropout率を指定する。全ての時間刻みで同じdropoutマスクがかかる。\n",
        "dropoutを使ってネットワークを正則化する際は、常に、完全に収束するのにより時間がかかるようになる為、ここではepoch数を2倍にする。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b073-ql6615y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras.optimizers import RMSprop\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.GRU(32,\n",
        "                     dropout=0.2,\n",
        "                     recurrent_dropout=0.2,\n",
        "                     input_shape=(None, float_data.shape[-1])))\n",
        "model.add(layers.Dense(1))\n",
        "\n",
        "model.compile(optimizer=RMSprop(), loss='mae')\n",
        "history = model.fit_generator(train_gen,\n",
        "                              steps_per_epoch=500,\n",
        "                              epochs=40,\n",
        "                              validation_data=val_gen,\n",
        "                              validation_steps=val_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqZW_LxF6152",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "loss = hist.history['loss']\n",
        "val_loss = hist.history['val_loss']\n",
        "\n",
        "# lossのグラフ\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(range(40), loss, marker='.', label='training_loss')\n",
        "plt.plot(range(40), val_loss, marker='.', label='validation_loss')\n",
        "plt.legend(loc='best', fontsize=10)\n",
        "plt.grid()\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzvjN4o86154",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpFIQNLT6156",
        "colab_type": "text"
      },
      "source": [
        "#### 1次元の畳み込みを使った学習\n",
        "\n",
        "ここまでが、RNNを使った時系列データ解析。\n",
        "教科書(PythonとKerasによるディープラーニング、Francois Chollet著)には他にも、GRUをスタックする方法、\n",
        "双方向RNNを用いる方法が載っているが、ここではもう一つの時系列データ解析手法である1D convolution層(1次元畳み込み層)を用いた手法を紹介する。\n",
        "データについて、これまで用いて来た気象データを用い、float_data、train_gen、val_gen、val_stepsを再利用する。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G70hjtlh6157",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D, Dense\n",
        "from keras.optimizers import RMSprop\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv1D(32, 5, activation='relu', input_shape=(None, float_data.shape[-1]))) \n",
        "model.add(MaxPooling1D(3))\n",
        "model.add(Conv1D(32, 5, activation='relu'))\n",
        "model.add(MaxPooling1D(3))\n",
        "model.add(Conv1D(32, 5, activation='relu'))\n",
        "model.add(GlobalMaxPooling1D()) #2Dconvolutionの時に用いていたFlattenではなくGlobalMaxPooling1Dを用いる。これで3次元出力が2次元になる。\n",
        "model.add(Dense(1))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ufvnpa66159",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer=RMSprop(), loss='mae')\n",
        "hist = model.fit_generator(train_gen,\n",
        "                           steps_per_epoch=500,\n",
        "                           epochs=20,\n",
        "                           validation_data=val_gen,\n",
        "                           validation_steps=val_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlMsbWsx616A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "loss = hist.history['loss']\n",
        "val_loss = hist.history['val_loss']\n",
        "\n",
        "# lossのグラフ\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(range(20), loss, marker='.', label='training_loss')\n",
        "plt.plot(range(20), val_loss, marker='.', label='validation_loss')\n",
        "plt.legend(loc='best', fontsize=10)\n",
        "plt.grid()\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcvwaCIW616C",
        "colab_type": "text"
      },
      "source": [
        "あまり性能が良くない。これはRNNが時系列データ(データの時間的変化)を捉えるのに対して、\n",
        "CNNでは時間的な順序に鈍感であり局所的なパターンを認識するため(特に気象データでは最近のデータが意味を持つ)。\n",
        "CNNでも層を深くすることでより長期間のパターンを認識できるようになるが、気象データのような時間的順序が意味を持つデータには向いていない。\n",
        "短い繰り返しパターンが続くデータなどの解析では1DCNNが威力を発揮すると考えられる。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-ml7LSk616D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}